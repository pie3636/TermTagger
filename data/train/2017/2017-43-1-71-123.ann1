Highly	O
frequent	O
in	O
language	O
and	O
communication	O
,	O
metaphor	B
represents	O
a	O
significant	O
challenge	O
for	O
Natural	B
Language	I
Processing	I
(	O
NLP	B
)	O
applications	O
.	O

Computational	O
work	O
on	O
metaphor	B
has	O
traditionally	O
evolved	O
around	O
the	O
use	O
of	O
hand	O
-	O
coded	O
knowledge	O
,	O
making	O
the	O
systems	O
hard	O
to	O
scale	O
.	O

Recent	O
years	O
have	O
witnessed	O
a	O
rise	O
in	O
statistical	B
approaches	I
to	O
metaphor	B
processing	I
.	O

However	O
,	O
these	O
approaches	O
often	O
require	O
extensive	O
human	O
annotation	B
effort	O
and	O
are	O
predominantly	O
evaluated	O
within	O
a	O
limited	O
domain	O
.	O

In	O
contrast	O
,	O
we	O
experiment	O
with	O
weakly	B
supervised	I
and	O
unsupervised	B
techniques	I
—	O
with	O
little	O
or	O
no	O
annotation	B
—	O
to	O
generalize	O
higher	O
-	O
level	O
mechanisms	O
of	O
metaphor	B
from	O
distributional	O
properties	O
of	O
concepts	O
.	O

We	O
investigate	O
different	O
levels	O
and	O
types	O
of	O
supervision	B
(	O
learning	O
from	O
linguistic	B
examples	O
vs.	O
learning	O
from	O
a	O
given	O
set	O
of	O
metaphorical	B
mappings	B
vs.	O
learning	O
without	O
annotation	B
)	O
in	O
flat	O
and	O
hierarchical	O
,	O
unconstrained	O
and	O
constrained	B
clustering	I
settings	O
.	O

Our	O
aim	O
is	O
to	O
identify	O
the	O
optimal	O
type	O
of	O
supervision	B
for	O
a	O
learning	O
algorithm	O
that	O
discovers	O
patterns	O
of	O
metaphorical	B
association	I
from	O
text	O
.	O

In	O
order	O
to	O
investigate	O
the	O
scalability	O
and	O
adaptability	O
of	O
our	O
models	B
,	O
we	O
applied	O
them	O
to	O
data	O
in	O
three	O
languages	O
from	O
different	O
language	O
groups	O
—	O
English	O
,	O
Spanish	O
,	O
and	O
Russian	O
—	O
achieving	O
state	B
-	I
of	I
-	I
the	I
-	I
art	I
results	O
with	O
little	O
supervision	B
.	O

Finally	O
,	O
we	O
demonstrate	O
that	O
statistical	B
methods	I
an	O
facilitate	O
and	O
scale	O
up	O
cross	O
-	O
linguistic	O
research	O
on	O
metaphor	B
.	O
