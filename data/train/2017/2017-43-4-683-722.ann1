We	O
present	O
novel	O
methods	O
for	O
analyzing	O
the	O
activation	B
patterns	I
of	O
recurrent	B
neural	I
networks	I
from	O
a	O
linguistic	O
point	O
of	O
view	O
and	O
explore	O
the	O
types	O
of	O
linguistic	B
structure	I
they	O
learn	O
.	O

As	O
a	O
case	O
study	O
,	O
we	O
use	O
a	O
standard	O
standalone	O
language	B
model	I
,	O
and	O
a	O
multi	O
-	O
task	O
gated	O
recurrent	B
network	I
architecture	O
consisting	O
of	O
two	O
parallel	B
pathways	I
with	O
shared	B
word	I
embeddings	I
:	O
The	O
Visual	O
pathway	O
is	O
trained	O
on	O
predicting	O
the	O
representations	O
of	O
the	O
visual	O
scene	O
corresponding	O
to	O
an	O
input	O
sentence	O
,	O
and	O
the	O
Textual	O
pathway	O
is	O
trained	O
to	O
predict	O
the	O
next	O
word	O
in	O
the	O
same	O
sentence	O
.	O

We	O
propose	O
a	O
method	O
for	O
estimating	O
the	O
amount	O
of	O
contribution	O
of	O
individual	O
tokens	B
in	O
the	O
input	O
to	O
the	O
final	O
prediction	O
of	O
the	O
networks	B
.	O

Using	O
this	O
method	O
,	O
we	O
show	O
that	O
the	O
Visual	O
pathway	O
pays	O
selective	O
attention	O
to	O
lexical	B
categories	I
and	O
grammatical	B
functions	I
that	O
carry	O
semantic	B
information	I
,	O
and	O
learns	O
to	O
treat	O
word	O
types	O
differently	O
depending	O
on	O
their	O
grammatical	B
function	I
and	O
their	O
position	O
in	O
the	O
sequential	B
structure	I
of	O
the	O
sentence	O
.	O

In	O
contrast	O
,	O
the	O
language	B
models	I
are	O
comparatively	O
more	O
sensitive	O
to	O
words	O
with	O
a	O
syntactic	B
function	I
.	O

Further	O
analysis	O
of	O
the	O
most	O
informative	O
n	B
-	I
gram	I
contexts	O
for	O
each	O
model	B
shows	O
that	O
in	O
comparison	O
with	O
the	O
VISUAL	O
pathway	O
,	O
the	O
language	B
models	I
react	O
more	O
strongly	O
to	O
abstract	O
contexts	O
that	O
represent	O
syntactic	B
constructions	I
.	O
