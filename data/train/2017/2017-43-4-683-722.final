We	O
present	O
novel	O
methods	B
for	O
analyzing	B
the	I
activation	I
patterns	I
of	O
recurrent	B
neural	I
networks	I
from	O
a	O
linguistic	B
point	I
of	I
view	I
and	O
explore	O
the	O
types	B
of	I
linguistic	I
structure	I
they	O
learn	O
.	O
	O
As	O
a	O
case	O
study	O
,	O
we	O
use	O
a	O
standard	B
standalone	I
language	I
model	I
,	O
and	O
a	O
multi	B
-	I
task	I
gated	I
recurrent	I
network	I
architecture	I
consisting	O
of	O
two	O
parallel	O
pathways	O
with	O
shared	O
word	B
embeddings	I
:	O
The	O
Visual	B
pathway	I
is	O
trained	B
on	O
predicting	B
the	I
representations	I
of	O
the	O
visual	O
scene	O
corresponding	O
to	O
an	O
input	B
sentence	I
,	O
and	O
the	O
Textual	B
pathway	I
is	O
trained	B
to	O
predict	B
the	I
next	I
word	I
in	O
the	O
same	O
sentence	B
.	O
	O
We	O
propose	O
a	O
method	B
for	O
estimating	O
the	O
amount	O
of	O
contribution	O
of	O
individual	O
tokens	B
in	O
the	O
input	B
to	O
the	O
final	B
prediction	I
of	O
the	O
networks	B
.	O
	O
Using	O
this	O
method	B
,	O
we	O
show	O
that	O
the	O
Visual	B
pathway	I
pays	O
selective	O
attention	O
to	O
lexical	B
categories	I
and	O
grammatical	B
functions	I
that	O
carry	B
semantic	I
information	I
,	O
and	O
learns	B
to	O
treat	O
word	B
types	O
differently	O
depending	O
on	O
their	O
grammatical	B
function	I
and	O
their	O
position	B
in	I
the	I
sequential	I
structure	I
of	O
the	O
sentence	O
.	O
	O
In	O
contrast	O
,	O
the	O
language	B
models	I
are	O
comparatively	O
more	O
sensitive	O
to	O
words	B
with	I
a	I
syntactic	I
function	I
.	O
	O
Further	O
analysis	O
of	O
the	O
most	O
informative	O
n	B
-	I
gram	I
contexts	I
for	O
each	O
model	B
shows	O
that	O
in	O
comparison	O
with	O
the	O
VISUAL	B
pathway	I
,	O
the	O
language	B
models	I
react	O
more	O
strongly	O
to	O
abstract	O
contexts	B
that	O
represent	B
syntactic	I
constructions	I
.	O
