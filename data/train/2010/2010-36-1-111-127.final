We	O
introduce	O
a	O
generative	B
probabilistic	I
model	I
,	O
the	O
noisy	B
channel	I
model	I
,	O
for	O
unsupervised	B
word	I
sense	I
disambiguation	I
.	O
In	O
our	O
model	O
,	O
each	O
context	O
C	O
is	O
modeled	O
as	O
a	O
distinct	O
channel	O
through	O
which	O
the	O
speaker	O
intends	O
to	O
transmit	O
a	O
particular	O
meaning	O
S	O
using	O
a	O
possibly	O
ambiguous	B
word	I
W.	O
To	O
reconstruct	O
the	O
intended	O
meaning	O
the	O
hearer	O
uses	O
the	O
distribution	O
of	O
possible	O
meanings	O
in	O
the	O
given	O
context	O
P(S|C	O
)	O
and	O
possible	O
words	O
that	O
can	O
express	O
each	O
meaning	O
P(W|S	O
)	O
.	O
We	O
assume	O
P(W|S	O
)	O
is	O
independent	O
of	O
the	O
context	O
and	O
estimate	O
it	O
using	O
WordNet	B
sense	I
frequencies	I
.	O
The	O
main	O
problem	O
of	O
unsupervised	B
WSD	I
is	O
estimating	O
context	B
-	I
dependent	I
P(S|C	O
)	O
without	O
access	O
to	O
any	O
sense	B
-	I
tagged	I
text	I
.	O
We	O
show	O
one	O
way	O
to	O
solve	O
this	O
problem	O
using	O
a	O
statistical	B
language	I
model	I
based	O
on	O
large	O
amounts	O
of	O
untagged	B
text	I
.	O
Our	O
model	O
uses	O
coarse	B
-	I
grained	I
semantic	I
classes	I
for	O
S	O
internally	O
and	O
we	O
explore	O
the	O
effect	O
of	O
using	O
different	O
levels	O
of	O
granularity	B
on	O
WSD	B
performance	I
.	O
The	O
system	O
outputs	O
fine	B
-	I
grained	I
senses	I
for	O
evaluation	O
,	O
and	O
its	O
performance	O
on	O
noun	B
disambiguation	I
is	O
better	O
than	O
most	O
previously	O
reported	O
unsupervised	B
systems	I
and	O
close	O
to	O
the	O
best	O
supervised	B
systems	I
.	O
