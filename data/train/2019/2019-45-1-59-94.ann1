This	O
article	O
describes	O
a	O
neural	B
semantic	I
parser	I
that	O
maps	O
natural	B
language	I
utterances	O
onto	O
logical	B
forms	I
that	O
can	O
be	O
executed	O
against	O
a	O
task	B
-	I
specific	I
environment	O
,	O
such	O
as	O
a	O
knowledge	O
base	O
or	O
a	O
database	B
,	O
to	O
produce	O
a	O
response	O
.	O
The	O
parser	B
generates	B
tree	B
-	I
structured	I
logical	O
forms	O
with	O
a	O
transition	O
-	O
based	O
approach	O
,	O
combining	O
a	O
generic	O
tree	B
-	I
generation	I
algorithm	I
with	O
domain	O
-	O
general	O
grammar	B
defined	O
by	O
the	O
logical	O
language	O
.	O
The	O
generation	B
process	I
is	O
modeled	O
by	O
structured	O
recurrent	O
neural	B
networks	I
,	O
which	O
provide	O
a	O
rich	O
encoding	B
of	O
the	O
sentential	B
context	I
and	O
generation	B
history	I
for	O
making	O
predictions	B
.	O
To	O
tackle	O
mismatches	B
between	O
natural	B
language	I
and	O
logical	B
form	O
tokens	I
,	O
various	O
attention	O
mechanisms	O
are	O
explored	O
.	O
Finally	O
,	O
we	O
consider	O
different	O
training	O
settings	O
for	O
the	O
neural	B
semantic	I
parser	I
,	O
including	O
fully	O
supervised	B
training	I
where	O
annotated	B
logical	I
forms	I
are	O
given	O
,	O
weakly	O
supervised	B
training	I
where	O
denotations	O
are	O
provided	O
,	O
and	O
distant B
supervision	I
where	O
only	O
unlabeled	B
sentences	I
and	O
a	O
knowledge	B
base	I
are	O
available	O
.	O
Experiments	B
across	O
a	O
wide	O
range	O
of	O
data	B
sets	I
demonstrate	O
the	O
effectiveness	O
of	O
our	O
parser	B
.	O