This	O
article	O
describes	O
a	O
neural	B
semantic	I
parser	I
that	O
maps	O
natural	B
language	I
utterances	O
onto	O
logical	O
forms	O
that	O
can	O
be	O
executed	O
against	O
a	O
task	O
-	O
specific	O
environment	O
,	O
such	O
as	O
a	O
knowledge	B
base	I
or	O
a	O
database	B
,	O
to	O
produce	O
a	O
response	O
.	O

The	O
parser	B
generates	O
tree	B
-	I
structured	I
logical	I
forms	I
with	O
a	O
transition	B
-	I
based	I
approach	I
,	O
combining	O
a	O
generic	O
tree	B
-	I
generation	I
algorithm	I
with	O
domain	B
-	I
general	I
grammar	I
defined	O
by	O
the	O
logical	B
language	I
.	O

The	O
generation	O
process	O
is	O
modeled	O
by	O
structured	O
recurrent	B
neural	I
networks	I
,	O
which	O
provide	O
a	O
rich	O
encoding	B
of	O
the	O
sentential	B
context	I
and	O
generation	O
history	O
for	O
making	O
predictions	O
.	O

To	O
tackle	O
mismatches	O
between	O
natural	B
language	I
and	O
logical	B
form	I
tokens	I
,	O
various	O
attention	B
mechanisms	I
are	O
explored	O
.	O

Finally	O
,	O
we	O
consider	O
different	O
training	O
settings	O
for	O
the	O
neural	B
semantic	I
parser	I
,	O
including	O
fully	O
supervised	B
training	I
where	O
annotated	O
logical	B
forms	I
are	O
given	O
,	O
weakly	O
supervised	B
training	I
where	O
denotations	B
are	O
provided	O
,	O
and	O
distant	B
supervision	I
where	O
only	O
unlabeled	O
sentences	O
and	O
a	O
knowledge	B
base	I
are	O
available	O
.	O

Experiments	O
across	O
a	O
wide	O
range	O
of	O
data	O
sets	O
demonstrate	O
the	O
effectiveness	O
of	O
our	O
parser	B
.	O