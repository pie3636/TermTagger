This	O
article	O
describes	O
a	O
neural	B
semantic	I
parser	I
that	O
maps	O
natural	B
language	I
utterances	B
onto	O
logical	B
forms	I
that	O
can	O
be	O
executed	O
against	O
a	O
task	O
-	O
specific	O
environment	O
,	O
such	O
as	O
a	O
knowledge	B
base	I
or	O
a	O
database	B
,	O
to	O
produce	O
a	O
response	O
.	O

The	O
parser	B
generates	O
tree	B
-	I
structured	B
logical	I
forms	I
with	O
a	O
transition	B
-	I
based	I
approach	I
,	O
combining	O
a	O
generic	B
tree	I
-	O
generation	B
algorithm	I
with	O
domain	B
-	O
general	B
grammar	I
defined	O
by	O
the	O
logical	B
language	I
.	O

The	O
generation	O
process	O
is	O
modeled	O
by	O
structured	B
recurrent	I
neural	I
networks	I
,	O
which	O
provide	O
a	O
rich	O
encoding	B
of	O
the	O
sentential	B
context	I
and	O
generation	O
history	O
for	O
making	O
predictions	O
.	O

To	O
tackle	O
mismatches	O
between	O
natural	B
language	I
and	O
logical	B
form	I
tokens	I
,	O
various	O
attention	B
mechanisms	I
are	O
explored	O
.	O

Finally	O
,	O
we	O
consider	O
different	O
training	B
settings	I
for	O
the	O
neural	B
semantic	I
parser	I
,	O
including	O
fully	B
supervised	I
training	I
where	O
annotated	B
logical	B
forms	I
are	O
given	O
,	O
weakly	B
supervised	I
training	I
where	O
denotations	B
are	O
provided	O
,	O
and	O
distant	B
supervision	I
where	O
only	O
unlabeled	B
sentences	I
and	O
a	O
knowledge	B
base	I
are	O
available	O
.	O

Experiments	O
across	O
a	O
wide	O
range	O
of	O
data	O
sets	O
demonstrate	O
the	O
effectiveness	O
of	O
our	O
parser	B
.	O