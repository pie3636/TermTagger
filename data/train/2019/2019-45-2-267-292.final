Neural	B
machine	I
translation	I
(	I
NMT	I
)	I
has	O
shown	O
great	O
success	O
as	O
a	O
new	O
alternative	O
to	O
the	O
traditional	O
Statistical	B
Machine	I
Translation	I
model	O
in	O
multiple	O
languages	O
.	O

Early	O
NMT	B
models	O
are	O
based	O
on	O
sequence	B
-	I
to	I
-	I
sequence	I
learning	I
that	O
encodes	B
a	O
sequence	O
of	O
source	O
words	O
into	O
a	O
vector	B
space	I
and	O
generates	O
another	O
sequence	O
of	O
target	O
words	O
from	O
the	O
vector	B
.	O

In	O
those	O
NMT	B
models	O
,	O
sentences	O
are	O
simply	O
treated	O
as	O
sequences	O
of	O
words	O
without	O
any	O
internal	O
structure	O
.	O

In	O
this	O
article	O
,	O
we	O
focus	O
on	O
the	O
role	O
of	O
the	O
syntactic	B
structure	I
of	O
source	O
sentences	O
and	O
propose	O
a	O
novel	O
end	B
-	I
to	I
-	I
end	I
syntactic	I
NMT	I
model	I
,	O
which	O
we	O
call	O
a	O
tree	B
-	I
to	I
-	I
sequence	I
NMT	I
model	I
,	O
extending	O
a	O
sequence	B
-	I
to	I
-	I
sequence	I
model	I
with	O
the	O
source	B
-	I
side	I
phrase	B
structure	I
.	O

Our	O
proposed	O
model	O
has	O
an	O
attention	B
mechanism	I
that	O
enables	O
the	O
decoder	B
to	O
generate	B
a	O
translated	O
word	O
while	O
softly	O
aligning	O
it	O
with	O
phrases	O
as	O
well	O
as	O
words	O
of	O
the	O
source	O
sentence	O
.	O

We	O
have	O
empirically	O
compared	O
the	O
proposed	O
model	O
with	O
sequence	B
-	I
to	I
-	I
sequence	I
models	I
in	O
various	O
settings	O
on	O
Chinese	O
-	O
to	O
-	O
Japanese	O
and	O
English	O
-	O
to	O
-	O
Japanese	O
translation	B
tasks	I
.	O

Our	O
experimental	O
results	O
suggest	O
that	O
the	O
use	O
of	O
syntactic	B
structure	I
can	O
be	O
beneficial	O
when	O
the	O
training	B
data	I
set	O
is	O
small	O
,	O
but	O
is	O
not	O
as	O
effective	O
as	O
using	O
a	O
bi	B
-	I
directional	I
encoder	I
.	O

As	O
the	O
size	O
of	O
training	B
data	I
set	I
increases	O
,	O
the	O
benefits	O
of	O
using	O
a	O
syntactic	B
tree	I
tends	O
to	O
diminish	O
.	O
