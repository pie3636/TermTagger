We	O
borrow	O
the	O
concept	O
of	O
representation	B
learning	I
from	O
deep	B
learning	I
research	O
,	O
and	O
we	O
argue	O
that	O
the	O
quest	O
for	O
Greenbergian	B
implicational	I
universals	I
can	O
be	O
reformulated	O
as	O
the	O
learning	O
of	O
good	O
latent	B
representations	I
of	O
languages	O
,	O
or	O
sequences	O
of	O
surface	B
typological	I
features	I
.	O

By	O
projecting	O
languages	O
into	O
latent	B
representations	I
and	O
performing	O
inference	B
in	O
the	O
latent	B
space	I
,	O
we	O
can	O
handle	O
complex	O
dependencies	O
among	O
features	O
in	O
an	O
implicit	O
manner	O
.	O

The	O
most	O
challenging	O
problem	O
in	O
turning	O
the	O
idea	O
into	O
a	O
concrete	O
computational	B
model	I
is	O
the	O
alarmingly	O
large	O
number	O
of	O
missing	O
values	O
in	O
existing	O
typological	B
databases	I
.	O

To	O
address	O
this	O
problem	O
,	O
we	O
keep	O
the	O
number	O
of	O
model	B
parameters	I
relatively	O
small	O
to	O
avoid	O
overfitting	B
,	O
adopt	O
the	O
Bayesian	B
learning	I
framework	O
for	O
its	O
robustness	O
,	O
and	O
exploit	O
phylogenetically	B
and/or	O
spatially	B
related	I
languages	I
as	O
additional	O
clues	O
.	O

Experiments	O
show	O
that	O
the	O
proposed	O
model	O
recovers	O
missing	O
values	O
more	O
accurately	O
than	O
others	O
and	O
that	O
some	O
latent	B
variables	O
exhibit	O
phylogenetic	B
and	O
spatial	O
signals	O
comparable	O
to	O
those	O
of	O
surface	O
features	O
.	O
