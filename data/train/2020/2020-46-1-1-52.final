Despite	O
the	O
recent	O
success	O
of	O
deep	B
neural	I
networks	I
in	O
natural	B
language	I
processing	I
and	O
other	O
spheres	O
of	O
artificial	B
intelligence	I
,	O
their	O
interpretability	B
remains	O
a	O
challenge	O
.	O

We	O
analyze	O
the	O
representations	B
learned	O
by	O
neural	B
machine	I
translation	I
(	O
NMT	B
)	O
models	B
at	O
various	O
levels	B
of	I
granularity	I
and	O
evaluate	O
their	O
quality	O
through	O
relevant	O
extrinsic	B
properties	I
.	O

In	O
particular	O
,	O
we	O
seek	O
answers	O
to	O
the	O
following	O
questions	O
:	O
(	O
i	O
)	O
How	O
accurately	O
is	O
word	B
structure	I
captured	O
within	O
the	O
learned	B
representations	I
,	O
which	O
is	O
an	O
important	O
aspect	O
in	O
translating	B
morphologically	B
rich	I
languages	I
?	O

(	O
ii	O
)	O
Do	O
the	O
representations	B
capture	O
long	B
-	I
range	I
dependencies	I
,	O
and	O
effectively	O
handle	O
syntactically	B
divergent	I
languages	I
?	O

(	O
iii	O
)	O
Do	O
the	O
representations	B
capture	O
lexical	B
semantics	I
?	O

We	O
conduct	O
a	O
thorough	O
investigation	O
along	O
several	O
parameters	O
:	O
(	O
i	O
)	O
Which	O
layers	B
in	O
the	O
architecture	B
capture	O
each	O
of	O
these	O
linguistic	B
phenomena	I
;	O
(	O
ii	O
)	O
How	O
does	O
the	O
choice	O
of	O
translation	B
unit	I
(	O
word	B
,	O
character	B
,	O
or	O
subword	B
unit	I
)	O
impact	O
the	O
linguistic	B
properties	I
captured	O
by	O
the	O
underlying	O
representations	B
?	O

(	O
iii	O
)	O
Do	O
the	O
encoder	B
and	O
decoder	B
learn	O
differently	O
and	O
independently	O
?	O

(	O
iv	O
)	O
Do	O
the	O
representations	B
learned	I
by	O
multilingual	B
NMT	I
models	I
capture	O
the	O
same	O
amount	O
of	O
linguistic	B
information	I
as	O
their	O
bilingual	B
counterparts	I
?	O

Our	O
data	B
-	I
driven	I
,	O
quantitative	O
evaluation	O
illuminates	O
important	O
aspects	O
in	O
NMT	B
models	I
and	O
their	O
ability	O
to	O
capture	O
various	O
linguistic	B
phenomena	I
.	O

We	O
show	O
that	O
deep	B
NMT	I
models	I
trained	B
in	O
an	O
end	B
-	I
to	I
-	I
end	I
fashion	O
,	O
without	O
being	O
provided	O
any	O
direct	B
supervision	I
during	O
the	O
training	B
process	I
,	O
learn	O
a	O
non	O
-	O
trivial	O
amount	O
of	O
linguistic	B
information	I
.	O

Notable	O
findings	O
include	O
the	O
following	O
observations	O
:	O
(	O
i	O
)	O
Word	B
morphology	I
and	O
part	B
-	I
of	I
-	I
speech	I
information	I
are	O
captured	O
at	O
the	O
lower	B
layers	I
of	O
the	O
model	B
;	O
(	O
ii	O
)	O
In	O
contrast	O
,	O
lexical	B
semantics	I
or	O
non	O
-	O
local	O
syntactic	B
and	I
semantic	B
dependencies	I
are	O
better	O
represented	O
at	O
the	O
higher	B
layers	I
of	O
the	O
model	B
;	O
(	O
iii	O
)	O
Representations	B
learned	I
using	O
characters	B
are	O
more	O
informed	O
about	O
word	B
-	I
morphology	I
compared	O
to	O
those	O
learned	O
using	O
subword	B
units	I
;	O
and	O
(	O
iv	O
)	O
Representations	B
learned	I
by	O
multilingual	B
models	I
are	O
richer	O
compared	O
to	O
bilingual	B
models	I
.	O

