Understanding	O
predictions	O
made	O
by	O
deep	B
neural	I
networks	I
is	O
notoriously	O
difficult	O
,	O
but	O
also	O
crucial	O
to	O
their	O
dissemination	O
.	O
As	O
all	O
machine	B
learning	I
â€“	I
based	I
methods	I
,	O
they	O
are	O
as	O
good	O
as	O
their	O
training	B
data	I
,	O
and	O
can	O
also	O
capture	O
unwanted	O
biases	O
.	O
While	O
there	O
are	O
tools	O
that	O
can	O
help	O
understand	O
whether	O
such	O
biases	O
exist	O
,	O
they	O
do	O
not	O
distinguish	O
between	O
correlation	O
and	O
causation	O
,	O
and	O
might	O
be	O
ill	O
-	O
suited	O
for	O
text	B
-	I
based	I
models	I
and	O
for	O
reasoning	O
about	O
high	B
-	I
level	I
language	I
concepts	I
.	O
A	O
key	O
problem	O
of	O
estimating	O
the	O
causal	O
effect	O
of	O
a	O
concept	O
of	O
interest	O
on	O
a	O
given	O
model	O
is	O
that	O
this	O
estimation	O
requires	O
the	O
generation	O
of	O
counterfactual	O
examples	O
,	O
which	O
is	O
challenging	O
with	O
existing	O
generation	B
technology	I
.	O
To	O
bridge	O
that	O
gap	O
,	O
we	O
propose	O
CausaLM	B
,	O
a	O
framework	O
for	O
producing	O
causal	B
model	I
explanations	I
using	O
counterfactual	B
language	I
representation	I
models	I
.	O
Our	O
approach	O
is	O
based	O
on	O
fine	O
-	O
tuning	O
of	O
deep	B
contextualized	I
embedding	I
models	I
with	O
auxiliary	B
adversarial	I
tasks	I
derived	O
from	O
the	O
causal	B
graph	I
of	O
the	O
problem	O
.	O
Concretely	O
,	O
we	O
show	O
that	O
by	O
carefully	O
choosing	O
auxiliary	B
adversarial	I
pre	I
-	I
training	I
tasks	I
,	O
language	B
representation	I
models	I
such	O
as	O
BERT	B
can	O
effectively	O
learn	O
a	O
counterfactual	B
representation	I
for	O
a	O
given	O
concept	O
of	O
interest	O
,	O
and	O
be	O
used	O
to	O
estimate	O
its	O
true	O
causal	O
effect	O
on	O
model	B
performance	I
.	O
A	O
byproduct	O
of	O
our	O
method	O
is	O
a	O
language	B
representation	I
model	I
that	O
is	O
unaffected	O
by	O
the	O
tested	O
concept	O
,	O
which	O
can	O
be	O
useful	O
in	O
mitigating	O
unwanted	O
bias	O
ingrained	O
in	O
the	O
data	O
.	O