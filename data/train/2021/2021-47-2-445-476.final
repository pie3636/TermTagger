We	O
consider	O
the	O
task	O
of	O
crosslingual	B
semantic	I
parsing	I
in	O
the	O
style	O
of	O
Discourse	B
Representation	I
Theory	I
(	O
DRT	B
)	O
where	O
knowledge	O
from	O
annotated	B
corpora	I
in	O
a	O
resource	O
-	O
rich	O
language	B
is	O
transferred	O
via	O
bitext	B
to	O
guide	O
learning	O
in	O
other	O
languages	O
.	O
We	O
introduce	O
ğ•Œniversal	B
Discourse	I
Representation	I
Theory	I
(	O
ğ•ŒDRT	B
)	O
,	O
a	O
variant	O
of	O
DRT	B
that	O
explicitly	O
anchors	O
semantic	B
representations	I
to	O
tokens	B
in	O
the	O
linguistic	B
input	I
.	O
We	O
develop	O
a	O
semantic	B
parsing	I
framework	I
based	O
on	O
the	O
Transformer	B
architecture	I
and	O
utilize	O
it	O
to	O
obtain	O
semantic	B
resources	I
in	O
multiple	B
languages	I
following	O
two	O
learning	O
schemes	O
.	O
The	O
many	B
-	I
to	I
-	I
one	I
approach	I
translates	O
non	O
-	O
English	O
text	O
to	O
English	O
,	O
and	O
then	O
runs	O
a	O
relatively	O
accurate	O
English	O
parser	B
on	O
the	O
translated	O
text	O
,	O
while	O
the	O
one	B
-	I
to	I
-	I
many	I
approach	I
translates	O
gold	B
standard	I
English	I
to	O
non	O
-	O
English	O
text	O
and	O
trains	B
multiple	O
parsers	B
(	O
one	O
per	O
language	O
)	O
on	O
the	O
translations	O
.	O
Experimental	O
results	O
on	O
the	O
Parallel	B
Meaning	I
Bank	I
show	O
that	O
our	O
proposal	O
outperforms	O
strong	B
baselines	I
by	O
a	O
wide	O
margin	O
and	O
can	O
be	O
used	O
to	O
construct	O
(	O
silver	B
-	I
standard	I
)	O
meaning	B
banks	I
for	O
99	O
languages	O
.	O