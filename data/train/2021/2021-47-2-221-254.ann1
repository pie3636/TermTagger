Weighted	B
finite	I
automata	I
(	O
WFAs	B
)	O
are	O
often	O
used	O
to	O
represent	O
probabilistic	B
models	I
,	O
such	O
as	O
n	B
-	I
gram	I
language	I
models	I
,	O
because	O
among	O
other	O
things	O
,	O
they	O
are	O
efficient	O
for	O
recognition	B
tasks	I
in	O
time	O
and	O
space	O
.	O
The	O
probabilistic	B
source	I
to	O
be	O
represented	B
as	O
a	O
WFA	B
,	O
however	O
,	O
may	O
come	O
in	O
many	O
forms	O
.	O
Given	O
a	O
generic	O
probabilistic	B
model	I
over	O
sequences	O
,	O
we	O
propose	O
an	O
algorithm	O
to	O
approximate	O
it	O
as	O
a	O
WFA	B
such	O
that	O
the	O
Kullback	B
-	I
Leibler	I
divergence	I
between	O
the	O
source	B
model	I
and	O
the	O
WFA	B
target	I
model	I
is	O
minimized	O
.	O
The	O
proposed	O
algorithm	O
involves	O
a	O
counting	B
step	I
and	O
a	O
difference	O
of	O
convex	B
optimization	I
step	I
,	O
both	O
of	O
which	O
can	O
be	O
performed	O
efficiently	O
.	O
We	O
demonstrate	O
the	O
usefulness	O
of	O
our	O
approach	O
on	O
various	O
tasks	O
,	O
including	O
distilling	B
n	B
-	I
gram	I
models	I
from	O
neural	B
models	I
,	O
building	O
compact	B
language	I
models	I
,	O
and	O
building	O
open	B
-	I
vocabulary	I
character	I
models	I
.	O
The	O
algorithms	O
used	O
for	O
these	O
experiments	O
are	O
available	O
in	O
an	O
open	O
-	O
source	O
software	O
library	O
.	O