This	O
article	O
investigates	O
the	O
effects	O
of	O
different	O
degrees	O
of	O
contextual	B
granularity	O
on	O
language	B
model	I
performance	O
.	O

It	O
presents	O
a	O
new	O
language	B
model	I
that	O
combines	O
clustering	B
and	O
half	B
-	I
contextualization	I
,	O
a	O
novel	O
representation	O
of	O
contexts	B
.	O

Half	B
-	I
contextualization	I
is	O
based	O
on	O
the	O
half	B
-	I
context	I
hypothesis	I
that	O
states	O
that	O
the	O
distributional	O
characteristics	O
of	O
a	O
word	O
or	O
bigram	B
are	O
best	O
represented	O
by	O
treating	O
its	O
context	B
distribution	O
to	O
the	O
left	O
and	O
right	O
separately	O
and	O
that	O
only	O
directionally	O
relevant	O
distributional	O
information	O
should	O
be	O
used	O
.	O

Clustering	B
is	O
achieved	O
using	O
a	O
new	O
clustering	B
algorithm	O
for	O
class	O
-	O
based	O
language	B
models	I
that	O
compares	O
favorably	O
to	O
the	O
exchange	B
algorithm	I
.	O

When	O
interpolated	O
with	O
a	O
Kneser	B
-	I
Ney	I
model	I
,	O
half	B
-	I
context	I
models	B
are	O
shown	O
to	O
have	O
better	O
perplexity	B
than	O
commonly	O
used	O
interpolated	O
n	B
-	I
gram	I
models	B
and	O
traditional	O
class	O
-	O
based	O
approaches	O
.	O

A	O
novel	O
,	O
fine	O
-	O
grained	O
,	O
context	O
-	O
specific	O
analysis	O
highlights	O
those	O
contexts	O
in	O
which	O
the	O
model	O
performs	O
well	O
and	O
those	O
which	O
are	O
better	O
treated	O
by	O
existing	O
non	O
-	O
class	O
-	O
based	O
models	O
.	O


