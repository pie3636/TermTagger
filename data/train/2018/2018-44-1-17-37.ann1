Probabilistic	B
finite	I
-	I
state	I
automata	I
are	O
a	O
formalism	B
that	O
is	O
widely	O
used	O
in	O
many	O
problems	O
of	O
automatic	B
speech	I
recognition	I
and	O
natural	B
language	I
processing	I
.	O

Probabilistic	B
finite	I
-	I
state	I
automata	I
are	O
closely	O
related	O
to	O
other	O
finite	B
-	I
state	I
models	I
as	O
weighted	B
finite	I
-	I
state	I
automata	I
,	O
word	B
lattices	I
,	O
and	O
hidden	B
Markov	I
models	I
.	O

Therefore	O
,	O
they	O
share	O
many	O
similar	O
properties	O
and	O
problems	O
.	O

Entropy	B
measures	O
of	O
finite	B
-	I
state	I
models	I
have	O
been	O
investigated	O
in	O
the	O
past	O
in	O
order	O
to	O
study	O
the	O
information	B
capacity	I
of	O
these	O
models	O
.	O

The	O
derivational	B
entropy	B
quantifies	O
the	O
uncertainty	O
that	O
the	O
model	O
has	O
about	O
the	O
probability	B
distribution	I
it	O
represents	O
.	O

The	O
derivational	B
entropy	B
in	O
a	O
finite	B
-	I
state	I
automaton	I
is	O
computed	O
from	O
the	O
probability	B
that	O
is	O
accumulated	O
in	O
all	O
of	O
its	O
individual	O
state	B
sequences	I
.	O

The	O
computation	O
of	O
the	O
entropy	B
from	O
a	O
weighted	B
finite	I
-	I
state	I
automaton	I
requires	O
a	O
normalized	O
model	O
.	O

This	O
article	O
studies	O
an	O
efficient	O
computation	O
of	O
the	O
derivational	O
entropy	B
of	O
left	B
-	I
to	I
-	I
right	I
probabilistic	I
finite	I
-	I
state	I
automata	I
,	O
and	O
it	O
introduces	O
an	O
efficient	O
algorithm	O
for	O
normalizing	B
weighted	B
finite	I
-	I
state	I
automata	I
.	O

The	O
efficient	O
computation	O
of	O
the	O
derivational	O
entropy	B
is	O
also	O
extended	O
to	O
continuous	O
hidden	B
Markov	I
models	I
.	O

