Machine	B
learning	I
,	O
including	O
neural	B
network	I
techniques	I
,	O
have	O
been	O
applied	O
to	O
virtually	O
every	O
domain	O
in	O
natural	B
language	I
processing	I
.	O
One	O
problem	O
that	O
has	O
been	O
somewhat	O
resistant	O
to	O
effective	O
machine	B
learning	I
solutions	O
is	O
text	B
normalization	I
for	O
speech	O
applications	O
such	O
as	O
text	B
-	I
to	I
-	I
speech	I
synthesis	I
(	O
TTS	O
)	O
.	O
In	O
this	O
application	O
,	O
one	O
must	O
decide	O
,	O
for	O
example	O
,	O
that	O
123	O
is	O
verbalized	O
as	O
one	O
hundred	O
twenty	O
three	O
in	O
123	O
pages	O
but	O
as	O
one	O
twenty	O
three	O
in	O
123	O
King	O
Ave	O
.	O
For	O
this	O
task	O
,	O
state	B
-	I
of	I
-	I
the	O
-	I
art	I
industrial	I
systems	I
depend	O
heavily	O
on	O
hand	O
-	O
written	O
language	B
-	I
specific	I
grammars	I
.	O
We	O
propose	O
neural	B
network	I
models	I
that	O
treat	O
text	B
normalization	I
for	O
TTS	O
as	O
a	O
sequence	B
-	I
to	I
-	I
sequence	I
problem	O
,	O
in	O
which	O
the	O
input	O
is	O
a	O
text	B
token	I
in	O
context	O
,	O
and	O
the	O
output	O
is	O
the	O
verbalization	O
of	O
that	O
token	O
.	O
We	O
find	O
that	O
the	O
most	O
effective	O
model	O
,	O
in	O
accuracy	O
and	O
efficiency	O
,	O
is	O
one	O
where	O
the	O
sentential	O
context	O
is	O
computed	O
once	O
and	O
the	O
results	O
of	O
that	O
computation	O
are	O
combined	O
with	O
the	O
computation	O
of	O
each	O
token	B
in	I
sequence	I
to	O
compute	O
the	O
verbalization	O
.	O
This	O
model	O
allows	O
for	O
a	O
great	O
deal	O
of	O
flexibility	O
in	O
terms	O
of	O
representing	O
the	O
context	O
,	O
and	O
also	O
allows	O
us	O
to	O
integrate	B
tagging	I
and	O
segmentation	B
into	O
the	O
process	O
.	O
These	O
models	O
perform	O
very	O
well	O
overall	O
,	O
but	O
occasionally	O
they	O
will	O
predict	O
wildly	O
inappropriate	O
verbalizations	O
,	O
such	O
as	O
reading	O
3	O
cm	O
as	O
three	O
kilometers	O
.	O
Although	O
rare	O
,	O
such	O
verbalizations	O
are	O
a	O
major	O
issue	O
for	O
TTS	B
applications	I
.	O
We	O
thus	O
use	O
finite	B
-	I
state	I
covering	I
grammars	I
to	O
guide	O
the	O
neural	B
models	I
,	O
either	O
during	O
training	B
and	O
decoding	B
,	O
or	O
just	O
during	O
decoding	B
,	O
away	O
from	O
such	O
“	O
unrecoverable	O
”	O
errors	O
.	O
Such	O
grammars	O
can	O
largely	O
be	O
learned	O
from	O
data	O
.	O