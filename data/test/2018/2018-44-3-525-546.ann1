Orthographic	B
similarities	I
across	O
languages	O
provide	O
a	O
strong	O
signal	O
for	O
unsupervised	B
probabilistic	I
transduction	I
(	O
decipherment	B
)	O
for	O
closely	O
related	O
language	B
pairs	I
.	O

The	O
existing	O
decipherment	B
models	I
,	O
however	O
,	O
are	O
not	O
well	O
suited	O
for	O
exploiting	O
these	O
orthographic	B
similarities	I
.	O

We	O
propose	O
a	O
log	B
-	I
linear	I
model	I
with	O
latent	B
variables	I
that	O
incorporates	O
orthographic	B
similarity	I
features	I
.	O

Maximum	B
likelihood	I
training	I
is	O
computationally	O
expensive	O
for	O
the	O
proposed	O
log	B
-	I
linear	I
model	I
.	O

To	O
address	O
this	O
challenge	O
,	O
we	O
perform	O
approximate	B
inference	I
via	O
Markov	B
chain	I
Monte	B
Carlo	I
sampling	I
and	O
contrastive	B
divergence	I
.	O

Our	O
results	O
show	O
that	O
the	O
proposed	O
log	B
-	I
linear	I
model	I
with	O
contrastive	B
divergence	I
outperforms	O
the	O
existing	O
generative	B
decipherment	I
models	I
by	O
exploiting	O
the	O
orthographic	B
features	I
.	O

The	O
model	B
both	O
scales	O
to	O
large	O
vocabularies	B
and	O
preserves	O
accuracy	B
in	O
low-	O
and	O
no	B
-	I
resource	I
contexts	I
.	O

